{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySsIPBRiorxK"
   },
   "source": [
    "**Dataset**\n",
    "labeled datasset collected from twitter (Lab 1 - Hate Speech.tsv)\n",
    "\n",
    "**Objective**\n",
    "classify tweets containing hate speech from other tweets. <br>\n",
    "0 -> no hate speech <br>\n",
    "1 -> contains hate speech <br>\n",
    "\n",
    "\n",
    "**Evaluation metric**\n",
    "macro f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "NUM_OF_EPOCHES = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 06:34:48.492732: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-25 06:34:48.504829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745552088.518104   52250 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745552088.521951   52250 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745552088.532904   52250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745552088.532924   52250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745552088.532926   52250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745552088.532927   52250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-25 06:34:48.536455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# NLTK libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Gensim library\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure NLTK resources are downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eXUPo3g4orxV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG8MkuvjorxX"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: search how to load the data from tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BYeqhp66orxY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>we are so selfish. #orlando #standwithorlando #pulseshooting #orlandoshooting #biggerproblems #selfish #heabreaking   #values #love #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>i get to see my daddy today!!   #80days #gettingfed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>ouch...junior is angryð#got7 #junior #yugyoem   #omg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>i am thankful for having a paner. #thankful #positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>its #friday! ð smiles all around via ig user: @user #cookies make people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>as we all know, essential oils are not made of chemicals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>#euro2016 people blaming ha for conceded goal was it fat rooney who gave away free kick knowing bale can hit them from there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>sad little dude..   #badday #coneofshame #cats #pissed #funny #laughs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>product of the day: happy man #wine tool  who's   it's the #weekend? time to open up &amp;amp; drink up!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>@user #tgif   #ff to my #gamedev #indiedev #indiegamedev #squad! @user @user @user @user @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>beautiful sign by vendor 80 for $45.00!! #upsideofflorida #shopalyssas   #love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>@user all #smiles when #media is   !! ðð #pressconference in #antalya #turkey ! sunday #throwback  love! ððâ¤ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>we had a great panel on the mediatization of the public service   #ica16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>happy father's day @user ðððð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>50 people went to nightclub to have a good night and 1 man's actions means those people are lost to their families forever #rip#orlando</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  \\\n",
       "id          \n",
       "1       0   \n",
       "2       0   \n",
       "3       0   \n",
       "4       0   \n",
       "5       0   \n",
       "6       0   \n",
       "7       0   \n",
       "8       0   \n",
       "9       0   \n",
       "10      0   \n",
       "11      0   \n",
       "12      0   \n",
       "13      0   \n",
       "14      1   \n",
       "15      1   \n",
       "16      0   \n",
       "17      0   \n",
       "18      1   \n",
       "19      0   \n",
       "20      0   \n",
       "21      0   \n",
       "22      0   \n",
       "23      0   \n",
       "24      1   \n",
       "25      0   \n",
       "26      0   \n",
       "27      0   \n",
       "28      0   \n",
       "29      0   \n",
       "30      0   \n",
       "\n",
       "                                                                                                                                              tweet  \n",
       "id                                                                                                                                                   \n",
       "1                                             @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "2                        @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "3                                                                                                                               bihday your majesty  \n",
       "4                                                              #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦  \n",
       "5                                                                                                            factsguide: society now    #motivation  \n",
       "6                                [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  \n",
       "7                                                                         @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦  \n",
       "8   the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl  \n",
       "9                                                            we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦  \n",
       "10                                                                                                 @user @user welcome here !  i'm   it's so #gr8 !  \n",
       "11                                   â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex  \n",
       "12            we are so selfish. #orlando #standwithorlando #pulseshooting #orlandoshooting #biggerproblems #selfish #heabreaking   #values #love #  \n",
       "13                                                                                              i get to see my daddy today!!   #80days #gettingfed  \n",
       "14                                                                         @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot  \n",
       "15                                            no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins  \n",
       "16                                                                                          ouch...junior is angryð#got7 #junior #yugyoem   #omg  \n",
       "17                                                                                            i am thankful for having a paner. #thankful #positive  \n",
       "18                                                                                                                            retweet if you agree!  \n",
       "19                                                                      its #friday! ð smiles all around via ig user: @user #cookies make people  \n",
       "20                                                                                       as we all know, essential oils are not made of chemicals.   \n",
       "21                    #euro2016 people blaming ha for conceded goal was it fat rooney who gave away free kick knowing bale can hit them from there.  \n",
       "22                                                                            sad little dude..   #badday #coneofshame #cats #pissed #funny #laughs  \n",
       "23                                             product of the day: happy man #wine tool  who's   it's the #weekend? time to open up &amp; drink up!  \n",
       "24                                                                                                  @user @user lumpy says i am a . prove it lumpy.  \n",
       "25                                                   @user #tgif   #ff to my #gamedev #indiedev #indiegamedev #squad! @user @user @user @user @user  \n",
       "26                                                                   beautiful sign by vendor 80 for $45.00!! #upsideofflorida #shopalyssas   #love  \n",
       "27                    @user all #smiles when #media is   !! ðð #pressconference in #antalya #turkey ! sunday #throwback  love! ððâ¤ï¸  \n",
       "28                                                                         we had a great panel on the mediatization of the public service   #ica16  \n",
       "29                                                                                                        happy father's day @user ðððð  \n",
       "30          50 people went to nightclub to have a good night and 1 man's actions means those people are lost to their families forever #rip#orlando  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Hate Speech.tsv\", sep= \"\\t\", index_col='id')\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights from the data to determine the preprossing needed:\n",
    "\n",
    "- mentions like @user\n",
    "- `#` remove\n",
    "- punctiuations \n",
    "- non words like ±!!! ððð (non ascii)\n",
    "- things like [2/2]\n",
    "- remove numbers \n",
    "\n",
    "\n",
    "In the case of # we have 2 options :\n",
    "- remove the whole # with the word after it \n",
    "- remove the # sign only \n",
    "\n",
    "In our case Because hashtags often hold valuable meaning (e.g., #gold, #forex, #michigan) so it can make difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to split the data before EDA helps maintain the integrity of the machine learning process, prevents data leakage, simulates real-world scenarios more accurately, and ensures reliable model performance evaluation on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = pd.concat([x_train, y_train], axis=1)\n",
    "test_combined = pd.concat([x_test, y_test], axis=1)\n",
    "\n",
    "train_combined.drop_duplicates(inplace=True)\n",
    "test_combined.drop_duplicates(inplace=True)\n",
    "X_train = train_combined.drop('label', axis=1)\n",
    "y_train = train_combined['label']\n",
    "X_test = test_combined.drop('label', axis=1)\n",
    "y_test = test_combined['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9414</th>\n",
       "      <td>@user @user would like to wish you a   #father's day :),  #family #dad #fathersday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17664</th>\n",
       "      <td>always enjoy life! and be grateful for what you have #mondaymotivation #2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>kayak, sup, snorkel, swim...whatever your pleasure, we'll put it together!  #alohabeachbus   #hawaii #explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21957</th>\n",
       "      <td>@user what do you think of #alexjones saying #draintheswamp? #populationcontrol  you had good videos with him. they got him.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>well i guess i can't join servers mcpe 15.0. #mcpe    #mcpc #weird</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              tweet\n",
       "id                                                                                                                                 \n",
       "9414                                            @user @user would like to wish you a   #father's day :),  #family #dad #fathersday \n",
       "17664                                                  always enjoy life! and be grateful for what you have #mondaymotivation #2016\n",
       "438                  kayak, sup, snorkel, swim...whatever your pleasure, we'll put it together!  #alohabeachbus   #hawaii #explore \n",
       "21957  @user what do you think of #alexjones saying #draintheswamp? #populationcontrol  you had good videos with him. they got him.\n",
       "16373                                                            well i guess i can't join servers mcpe 15.0. #mcpe    #mcpc #weird"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqWVKi_GorxZ"
   },
   "source": [
    "### EDA on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1zxJpFxorxa"
   },
   "source": [
    "- check NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HVEttSujorxa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet    0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwjbzVaIorxb"
   },
   "source": [
    "- check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "J_FlBWISorxb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjIBFc35orxc"
   },
   "source": [
    "- show a representative sample of data texts to find out required preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zGFKzSCRorxc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9414</th>\n",
       "      <td>@user @user would like to wish you a   #father's day :),  #family #dad #fathersday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17664</th>\n",
       "      <td>always enjoy life! and be grateful for what you have #mondaymotivation #2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>kayak, sup, snorkel, swim...whatever your pleasure, we'll put it together!  #alohabeachbus   #hawaii #explore</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21957</th>\n",
       "      <td>@user what do you think of #alexjones saying #draintheswamp? #populationcontrol  you had good videos with him. they got him.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>well i guess i can't join servers mcpe 15.0. #mcpe    #mcpc #weird</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12586</th>\n",
       "      <td>trumps associates are a classy bunch.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>now playing #intwine -   on classic rock attic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>if lewis remains perfect, the rangers announcers are gonna stroke out.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>cool treat on a hot hot day ð§ #widn   #frozenyogu tuesday #dclife #summer #gogreenâ¦</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>actually ordered a kylie lip kit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>when i say fancy, you say food show! fancy! @user #sffs #countdown #sffs16 #foodshow #cantwait</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220</th>\n",
       "      <td>could it be that #systemicracism is impossible? real acts of racism is easier to stand up to  ...unfounately, youâ¦</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16116</th>\n",
       "      <td>good morning new york!!! beside myself with excitement! in croton on hudson. the most beautiful house, with beautiful people!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12757</th>\n",
       "      <td>ð   #music bogan will be a tribute to them says director !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23122</th>\n",
       "      <td>hey sis girls weekend hope your not in the #porsche</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28432</th>\n",
       "      <td>@user #charity #fundraising director comments \"i've previously used 6 systems but junaricrm+ is the most comprehensive and simplesâ¦</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27743</th>\n",
       "      <td>@user #karvi  #upcoming romance sequence  #super #duper   âð #yippee.! ðð #yehvaadaraha @user segment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21836</th>\n",
       "      <td>#rainbowrowell   bull up: you will dominate your bull and you will direct it whatever you want it to do. whe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19171</th>\n",
       "      <td>i am adventure. #i_am #positive #affirmation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9340</th>\n",
       "      <td>mrrat395:   #starfightersaturday phantomaviation cptplanespotter blackmaverick12 wbruchal ufeellucky spad5â¦</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        tweet  \\\n",
       "id                                                                                                                                              \n",
       "9414                                                      @user @user would like to wish you a   #father's day :),  #family #dad #fathersday    \n",
       "17664                                                            always enjoy life! and be grateful for what you have #mondaymotivation #2016   \n",
       "438                            kayak, sup, snorkel, swim...whatever your pleasure, we'll put it together!  #alohabeachbus   #hawaii #explore    \n",
       "21957            @user what do you think of #alexjones saying #draintheswamp? #populationcontrol  you had good videos with him. they got him.   \n",
       "16373                                                                      well i guess i can't join servers mcpe 15.0. #mcpe    #mcpc #weird   \n",
       "12586                                                                                                   trumps associates are a classy bunch.   \n",
       "5045                                                                                           now playing #intwine -   on classic rock attic   \n",
       "448                                                                  if lewis remains perfect, the rangers announcers are gonna stroke out.     \n",
       "2339                                                 cool treat on a hot hot day ð§ #widn   #frozenyogu tuesday #dclife #summer #gogreenâ¦   \n",
       "636                                                                                                          actually ordered a kylie lip kit   \n",
       "8701                                         when i say fancy, you say food show! fancy! @user #sffs #countdown #sffs16 #foodshow #cantwait     \n",
       "20220                   could it be that #systemicracism is impossible? real acts of racism is easier to stand up to  ...unfounately, youâ¦    \n",
       "16116         good morning new york!!! beside myself with excitement! in croton on hudson. the most beautiful house, with beautiful people!     \n",
       "12757                                                                           ð   #music bogan will be a tribute to them says director !   \n",
       "23122                                                                                     hey sis girls weekend hope your not in the #porsche   \n",
       "28432   @user #charity #fundraising director comments \"i've previously used 6 systems but junaricrm+ is the most comprehensive and simplesâ¦   \n",
       "27743                        @user #karvi  #upcoming romance sequence  #super #duper   âð #yippee.! ðð #yehvaadaraha @user segment   \n",
       "21836                            #rainbowrowell   bull up: you will dominate your bull and you will direct it whatever you want it to do. whe   \n",
       "19171                                                                                            i am adventure. #i_am #positive #affirmation   \n",
       "9340                            mrrat395:   #starfightersaturday phantomaviation cptplanespotter blackmaverick12 wbruchal ufeellucky spad5â¦   \n",
       "\n",
       "       label  \n",
       "id            \n",
       "9414       0  \n",
       "17664      0  \n",
       "438        0  \n",
       "21957      1  \n",
       "16373      0  \n",
       "12586      1  \n",
       "5045       0  \n",
       "448        0  \n",
       "2339       0  \n",
       "636        0  \n",
       "8701       0  \n",
       "20220      1  \n",
       "16116      0  \n",
       "12757      0  \n",
       "23122      0  \n",
       "28432      0  \n",
       "27743      0  \n",
       "21836      0  \n",
       "19171      0  \n",
       "9340       0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqdSUtbdorxd"
   },
   "source": [
    "- check dataset balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    21834\n",
       "1     1600\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of the labels\n",
    "train_combined['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Label Distribution'}, xlabel='label'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_combined['label'].value_counts().plot(kind='bar', title='Label Distribution', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3wl_crkorxd"
   },
   "source": [
    "- Cleaning and Preprocessing are:\n",
    "    - 1\n",
    "    - 2\n",
    "    - 3\n",
    "    - ... etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJkqK9gorxe"
   },
   "source": [
    "### Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extra: use custom scikit-learn Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using custom transformers in scikit-learn provides flexibility, reusability, and control over the data transformation process, allowing you to seamlessly integrate with scikit-learn's pipelines, enabling you to combine multiple preprocessing steps and modeling into a single workflow. This makes your code more modular, readable, and easier to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### link: https://www.andrewvillazon.com/custom-scikit-learn-transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GVjzzLhworxe"
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, \n",
    "                 fix_encoding=True, \n",
    "                 remove_hashtags=True, \n",
    "                 remove_mentions=True, \n",
    "                 remove_punctuation=True, \n",
    "                 lowercase=True, \n",
    "                 remove_stopwords=True, \n",
    "                 remove_urls=True, \n",
    "                 lemmatize=False, \n",
    "                 stem=True,\n",
    "                 replace_slang=True,\n",
    "                 remove_numbers=True,\n",
    "                 remove_stock_symbols=True):\n",
    "        \n",
    "        self.fix_encoding = fix_encoding\n",
    "        self.remove_hashtags = remove_hashtags\n",
    "        self.remove_mentions = remove_mentions\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_urls = remove_urls\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stem = stem\n",
    "        self.replace_slang = replace_slang\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.remove_stock_symbols = remove_stock_symbols\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return ''\n",
    "        \n",
    "        if self.fix_encoding:\n",
    "            text = self._fix_encoding(text)\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.replace_slang:\n",
    "            text = self._replace_slang(text)\n",
    "        if self.remove_urls:\n",
    "            text = self._remove_urls(text)\n",
    "        if self.remove_hashtags:\n",
    "            text = self._remove_hashtags(text)\n",
    "        if self.remove_mentions:\n",
    "            text = self._remove_mentions(text)\n",
    "        if self.remove_punctuation:\n",
    "            text = self._remove_punctuation(text)\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        if self.remove_stock_symbols:\n",
    "            text = re.sub(r'\\$\\w+', '', text)\n",
    "\n",
    "        words = text.split()\n",
    "        if self.remove_stopwords:\n",
    "            words = [w for w in words if w not in self.stop_words]\n",
    "        if self.lemmatize:\n",
    "            words = [self.lemmatizer.lemmatize(w) for w in words]\n",
    "        if self.stem:\n",
    "            words = [self.stemmer.stem(w) for w in words]\n",
    "\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def _fix_encoding(self, text):\n",
    "        return text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    def _replace_slang(self, text):\n",
    "        slang_dict = {\n",
    "            r'\\bur\\b': 'your',\n",
    "            r'\\bu\\b': 'you',\n",
    "            r'\\bgr8\\b': 'great',\n",
    "            r'\\b4u\\b': 'for you',\n",
    "            # r'\\blol\\b': 'laughing out loud'\n",
    "        }\n",
    "        for slang, replacement in slang_dict.items():\n",
    "            text = re.sub(slang, replacement, text)\n",
    "        return text\n",
    "    \n",
    "\n",
    "    def _remove_urls(self, text):\n",
    "        return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    def _remove_hashtags(self, text):\n",
    "        return re.sub(r'#', '', text)\n",
    "    \n",
    "    def _remove_mentions(self, text):\n",
    "        return re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    def _remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u2EYQvrBorxf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23434.000000\n",
       "mean        13.522275\n",
       "std         20.073546\n",
       "min          1.000000\n",
       "25%          9.000000\n",
       "50%         13.000000\n",
       "75%         17.000000\n",
       "90%         21.000000\n",
       "95%         23.000000\n",
       "99%         26.000000\n",
       "max       2607.000000\n",
       "Name: tweet, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined['tweet'].apply(lambda x: len(x.split())).describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### most of the sentences length is 26 word and there is an outline with 2607 \n",
    "\n",
    "- I will set max_len = 30 because it is a good balance as it:\n",
    "\n",
    "1. Covers nearly all data\n",
    "\n",
    "2. Avoids wasting memory on rare long outliers\n",
    "\n",
    "3. Keeps model efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_words=50_000, oov_token=\"<OOV>\"):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.tokenizer = Tokenizer(num_words=self.num_words, oov_token=self.oov_token)\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba0r1ASHorxf"
   },
   "source": [
    "**You  are doing Great so far!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9BhRQbYorxf"
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au1FYDPzorxg"
   },
   "source": [
    "#### Extra: use scikit-learn pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### link: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pipelines in scikit-learn promotes better code organization, reproducibility, and efficiency in machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP5FZzmborxg"
   },
   "source": [
    "#### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextTokenizer(num_words=50_000, oov_token=\"<OOV>\")\n",
    "X_train_tokenized = tokenizer.fit_transform(X_train['tweet'])\n",
    "X_test_tokenized = tokenizer.transform(X_test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m             embedding_matrix[i] \u001b[38;5;241m=\u001b[39m word2vec[word]\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding_matrix\n\u001b[0;32m---> 50\u001b[0m glove_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglove.6B.100d.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m fasttext_embeddings \u001b[38;5;241m=\u001b[39m load_fasttext_embeddings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcc.en.300.vec\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer)\n",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m, in \u001b[0;36mload_glove_embeddings\u001b[0;34m(file_path, tokenizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_glove_embeddings\u001b[39m(file_path, tokenizer):\n\u001b[1;32m      2\u001b[0m     embeddings_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(f, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading GloVe embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m             values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(file_path, tokenizer):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe embeddings\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    word_index = word_index = tokenizer.tokenizer.word_index\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_fasttext_embeddings(file_path, tokenizer):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for line in tqdm(f, desc=\"Loading FastText embeddings\"):\n",
    "            values = line.split()\n",
    "            word = values[0].decode('utf-8')\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    word_index = word_index = tokenizer.tokenizer.word_index\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_word2vec_embeddings(file_path, tokenizer):\n",
    "    word2vec = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "    word_index = word_index = tokenizer.tokenizer.word_index\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if word in word2vec:\n",
    "            embedding_matrix[i] = word2vec[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.100d.txt', tokenizer)\n",
    "\n",
    "fasttext_embeddings = load_fasttext_embeddings('cc.en.300.vec', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# model = LogisticRegression()\n",
    "\n",
    "# # Create the pipeline\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessing', CustomTransformer()),\n",
    "#     ('Vectorizing', Vectorizer()),\n",
    "#     ('model', model),\n",
    "# ])\n",
    "\n",
    "# # Now you can use the pipeline for training and prediction\n",
    "# # pipeline.fit(X_train, y_train)\n",
    "# # pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('Vectorizing', TfidfVectorizer()),\n",
    "    ('model', model),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, TextPreprocessor()),\n",
       "                (&#x27;Vectorizing&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, TextPreprocessor()),\n",
       "                (&#x27;Vectorizing&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(max_iter=1000))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TextPreprocessor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>TextPreprocessor()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing', TextPreprocessor()),\n",
       "                ('Vectorizing', TfidfVectorizer()),\n",
       "                ('model', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(df_train['tweet'], df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(df_test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      5875\n",
      "           1       0.89      0.31      0.46       432\n",
      "\n",
      "    accuracy                           0.95      6307\n",
      "   macro avg       0.92      0.65      0.72      6307\n",
      "weighted avg       0.95      0.95      0.94      6307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test['label'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note :`\n",
    "### Impact of Class Weights:\n",
    "- Class Weight balanced: This helps the model give more importance to the minority class (label=1), making it more likely to predict the minority class, thus improving recall for that class.\n",
    "\n",
    "- Without class weights: The model is biased towards the majority class (label=0), leading to a very high recall for the majority class and very low recall for the minority class, which was observed in your earlier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85JlkIQXorxg"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metric:**\n",
    "macro f1 score\n",
    "\n",
    "Macro F1 score is a useful metric in scenarios where you want to evaluate the overall performance of a multi-class classification model, **particularly when the classes are imbalanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Calculation](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/639c3d934e82c1195cdf3c60_macro-f1.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.7153906878335893\n"
     ]
    }
   ],
   "source": [
    "# macro f1 score\n",
    "f1_macro = f1_score(df_test['label'], predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LxBF_HEsorxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      5875\n",
      "           1       0.56      0.81      0.67       432\n",
      "\n",
      "    accuracy                           0.94      6307\n",
      "   macro avg       0.78      0.88      0.82      6307\n",
      "weighted avg       0.96      0.94      0.95      6307\n",
      "\n",
      "Macro F1 Score: 0.818105785314645\n"
     ]
    }
   ],
   "source": [
    "# Try with class weights\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('Vectorizing', TfidfVectorizer()),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "\n",
    "predictions = pipeline.predict(df_test['tweet'])\n",
    "\n",
    "print(classification_report(df_test['label'], predictions))\n",
    "\n",
    "# macro f1 score\n",
    "f1_macro = f1_score(df_test['label'], predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", f1_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhVFUaIcorxh"
   },
   "source": [
    "### Enhancement\n",
    "\n",
    "- Using different vectorizers with different hyperparameters\n",
    "- Trying different ML models and doing hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note`\n",
    "\n",
    "- You cannot use Word2Vec directly in the pipeline because it doesn't follow the Scikit-learn interface required for pipeline steps. Specifically, Scikit-learn expects each step in the pipeline (except the final estimator) to be a transformer that implements the fit and transform methods.\n",
    "\n",
    "- However, Word2Vec does not have a transform() method. It has a `wv` attribute to access word vectors, but there is no direct way to transform text into vectors using a simple function like transform().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [tweet.split() for tweet in X]  # Tokenize each tweet\n",
    "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for tweet in X:\n",
    "            words = [self.model.wv[word] for word in tweet.split() if word in self.model.wv]\n",
    "            if words:\n",
    "                tweet_vec = np.mean(words, axis=0)\n",
    "            else:\n",
    "                tweet_vec = np.zeros(self.vector_size)  # Handle empty tweets\n",
    "            embeddings.append(tweet_vec)\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Y4h1Danvorxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.72      0.83      5875\n",
      "           1       0.17      0.79      0.28       432\n",
      "\n",
      "    accuracy                           0.72      6307\n",
      "   macro avg       0.58      0.75      0.55      6307\n",
      "weighted avg       0.92      0.72      0.79      6307\n",
      "\n",
      "Macro F1 Score: 0.5546719058287899\n"
     ]
    }
   ],
   "source": [
    "# Try using Word2Vec for vectorization\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('Vectorizing', Word2VecTransformer()),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "\n",
    "predictions = pipeline.predict(df_test['tweet'])\n",
    "\n",
    "print(classification_report(df_test['label'], predictions))\n",
    "\n",
    "# macro f1 score\n",
    "f1_macro = f1_score(df_test['label'], predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try Glove\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from gensim.models import KeyedVectors\n",
    "# import numpy as np\n",
    "\n",
    "# class GloVeTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, vector_size=100):\n",
    "#         self.glove_file = 'glove.6B/glove.6B.100d.txt'  # Path to your GloVe file\n",
    "\n",
    "#         self.vector_size = vector_size\n",
    "#         self.word_vectors = None\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         # Load GloVe word vectors\n",
    "#         self.word_vectors = KeyedVectors.load_word2vec_format(self.glove_file, binary=False)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         embeddings = []\n",
    "#         for tweet in X:\n",
    "#             words = [self.word_vectors[word] for word in tweet.split() if word in self.word_vectors]\n",
    "#             if words:\n",
    "#                 tweet_vec = np.mean(words, axis=0)\n",
    "#             else:\n",
    "#                 tweet_vec = np.zeros(self.vector_size)  # Handle empty tweets\n",
    "#             embeddings.append(tweet_vec)\n",
    "#         return np.array(embeddings)\n",
    "    \n",
    "# # Example usage\n",
    "# # glove_file = 'glove.6B/glove.6B.100d.txt'  # Path to your GloVe file\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessing', TextPreprocessor()),\n",
    "#     ('Vectorizing', GloVeTransformer()),\n",
    "#     ('model', model),\n",
    "# ])\n",
    "\n",
    "# pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "\n",
    "# predictions = pipeline.predict(df_test['tweet'])\n",
    "\n",
    "# print(classification_report(df_test['label'], predictions))\n",
    "\n",
    "# # macro f1 score\n",
    "# f1_macro = f1_score(df_test['label'], predictions, average='macro')\n",
    "# print(\"Macro F1 Score:\", f1_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.70      0.82      5875\n",
      "           1       0.17      0.82      0.28       432\n",
      "\n",
      "    accuracy                           0.71      6307\n",
      "   macro avg       0.57      0.76      0.55      6307\n",
      "weighted avg       0.93      0.71      0.78      6307\n",
      "\n",
      "Macro F1 Score: 0.5455017009546693\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='linear', class_weight='balanced', max_iter=1000)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('Vectorizing', Word2VecTransformer()),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "\n",
    "predictions = pipeline.predict(df_test['tweet'])\n",
    "\n",
    "print(classification_report(df_test['label'], predictions))\n",
    "\n",
    "# macro f1 score\n",
    "f1_macro = f1_score(df_test['label'], predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class BagOfWordsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform(X).toarray()\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      5875\n",
      "           1       0.63      0.77      0.69       432\n",
      "\n",
      "    accuracy                           0.95      6307\n",
      "   macro avg       0.81      0.87      0.83      6307\n",
      "weighted avg       0.96      0.95      0.96      6307\n",
      "\n",
      "Macro F1 Score: 0.833624942665119\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('Vectorizing', BagOfWordsTransformer()),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "\n",
    "predictions = pipeline.predict(df_test['tweet'])\n",
    "\n",
    "print(classification_report(df_test['label'], predictions))\n",
    "# macro f1 score\n",
    "f1_macro = f1_score(df_test['label'], predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9239 - loss: 0.3007\n",
      "Epoch 2/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9319 - loss: 0.1695\n",
      "Epoch 3/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1234\n",
      "Epoch 4/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.1064\n",
      "Epoch 5/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.0948\n",
      "Epoch 6/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9643 - loss: 0.0870\n",
      "Epoch 7/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.0844\n",
      "Epoch 8/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.0861\n",
      "Epoch 9/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9632 - loss: 0.0865\n",
      "Epoch 10/10\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.0886\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      5875\n",
      "           1       0.79      0.61      0.69       432\n",
      "\n",
      "    accuracy                           0.96      6307\n",
      "   macro avg       0.88      0.80      0.83      6307\n",
      "weighted avg       0.96      0.96      0.96      6307\n",
      "\n",
      "Macro F1 Score: 0.8328053548942939\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df_train['tweet'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train['tweet'])\n",
    "X_test = tokenizer.texts_to_sequences(df_test['tweet'])\n",
    "\n",
    "# Padding\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_test = pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "y_train = df_train['label']\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=100))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, predictions))\n",
    "f1_macro = f1_score(y_test, predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", f1_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and final results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      5875\n",
      "           1       0.63      0.77      0.69       432\n",
      "\n",
      "    accuracy                           0.95      6307\n",
      "   macro avg       0.81      0.87      0.83      6307\n",
      "weighted avg       0.96      0.95      0.96      6307\n",
      "\n",
      "Macro F1 Score: 0.833624942665119\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('Vectorizing', BagOfWordsTransformer()),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "\n",
    "predictions = pipeline.predict(df_test['tweet'])\n",
    "\n",
    "print(classification_report(df_test['label'], predictions))\n",
    "# macro f1 score\n",
    "f1_macro = f1_score(df_test['label'], predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", f1_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note :` \n",
    "- That is the best result the best model and the vectorizer \n",
    "- Or the CNN model too \n",
    "\n",
    "#### The Results :\n",
    "- The model performs excellently in classifying the `majority` class `0`, with high precision, recall, and F1-score. However, the performance for the `minority` class `1` is weaker, with a lower precision and F1-score. This discrepancy could be due to class imbalance, as class 0 has significantly more instances than class 1. The overall accuracy and macro F1 score indicate that the model is generally effective but could benefit from techniques to improve classification for the minority class, such as oversampling, undersampling, or using advanced class-balancing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw1GVnYLorxi"
   },
   "source": [
    "#### Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
